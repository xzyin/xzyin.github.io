---
layout: post
title: INFO-NCE远离
description: "以House Price为例快速构建一个Base Line模型"
modified: 2020-10-01
tags: [Contrastive Learning]
categories: [算法]
image:
  path: /images/algorthim/info_nce/1.jpeg
  feature: /algorthim/info_nce/1.jpeg
  credit: x1
  creditlink: https://cxl.com/blog/bandit-tests/
---


到底什么是NCE？

# 从KL散度到交叉熵

Loss 函数的目的:

假设真实分布为P, 模型训练的到的分布是Q, 训练的目的就是为了是Q分布更接近P分布。

为了计算P分布和Q分布的距离，有什么办法呢？

$$D(P||Q)=\sum_{i=1}^n P(x) log\frac{P(x)}{Q(x)} \
= \sum_{i=1}^n P(x)log P(x) - \sum_{i=1}^n P(x)log Q(x)
$$

对于模型训练过程来说，真实分布是固定的，对于上面的公式只得到了一个:

$$D(P||Q)= - \sum_{i=1}^n P(x)log\;Q(x)
$$

如果需要最小化损失的话，只需要最小化上面这个公式即可。

# 从交叉熵和softmax结合

对于某条离散样本计算交叉熵的过程如下:

* 二分类的情况

我们用$\hat y$ 为正样本的预测概率, $y$ 为样本的真实标签值。在二分类问题中$\sum_{i=1}^2P(x)log\;Q(x)$只有两种情况。最终得到的结果:

$$D(P||Q)= -(y\;log(1-\hat y) + (1-y)\;log\;(1-\hat{y}))
$$

对手训练集上的样本, 对整体的交叉熵求平均。

* 多分类的情况

多分类情况下使用softmax计算每个样本的预测概率，每个样本计算出来的概率值我们表示为$E$

$$D(P||Q)= - \sum_{i=1}^n P(x)log\;Q(x)$$

公式可以描述为:

$$D(P||Q)= - \sum_{i=1}^n P(x)log\;\frac{E_i}{\sum_{j=1}^n E_j}$$

在假的类上P(x)=0最后得到的概率只是真类上的概率和，假类在softmax的分母中起作用，对上面的公式简化为:

$$D(P||Q)= - \sum_{i=1}^n log\;\frac{E^{pos}_j}{\sum_{j=1}^n E_j}$$

最小化上述公式等价于最小化:

$$D(P||Q)= - log\;\frac{E^{pos}_j}{\sum_{j=1}^n E_j}$$

所以在这里要搞清楚一个问题, 负采样是采样一次分类里面的正例和负例, 而不是正样本和负样本。在二分类问题中，我们预测点击的概率, 但是在多分类问题中, 我们预测的是每个类别上的概率。

我们的结果是:[1, 0, 0, 0, 0, 0, 0, 0]。

每条样本下都有一个真类和假类。也就是一条样本中包含真, 也包含假;不像二分类的情况以数据行来区分正样本和负样本。这篇文章[《候选采样（candidate sampling）》](https://zhuanlan.zhihu.com/p/75971908)也是值得一看。


# 从多分类到NCE

在上面的代码中, 有一个很困难的问题。计算softmax的loss的时候需要计算归一化的常数
$Z = \sum_{j=1}^n E_j$。对于超分类问题来说$n$的数目非常大，这里带来的计算量是致命的。在NCE中就是将原来的多分类问题转成了二分类问题。具体NCE为什么对真实分布的模拟是有效的我们后面再说。
在nce-loss采用中使用log-uniform分布来采样噪声分布的节点结果。nce-loss采样的函数如下:

$$P(class)=\frac{log(class + 2)-log(class + 1)}{log}$$



# 从representation到CPC
在Auto Encoder里面对于item或者词汇的embedding表示是基于label进行的。
以item2vec为例, 例如只有两个物品:
1. 物品A是美妆类型的视频, 嵌入得到了一个Embedding A1.

这个时候，在AutoEncoder的Encoder和Decoder只是拟合了一种特定的关系，这种特定的关系无法被其他模型学习到。也就是Embedding A不能代表A的特性。
或者换一种说法Embedding A即使能够代表A的特性，但是这种特性只适合在AutoEncoder的场景下使用。这种情况下的representation是not predictive的。

为了解决这个问题，有一种方式就是将物品A和Embedding A1以及组成一个pair对，将物品B和Embedding B1组成pair对，将物品A和Embedding B1组成pair对。再将物品B和embedding A1组成pair对，
去预测embedding和item是否匹配。

这种情况下实际上是有两个任务: 一个任务用来约束两个变量是否匹配, 一个任务用来训练模型。另外一个任务用来约束输入和embedding的关系，确保向量是predictive的。





# 从NCE到INFO NCE
在INFO NCE中采用一个log-uniform分布来模拟噪声分布。在真实环境当中, 怎么样试图通过负样本的构建来还原真实分布是至关重要的事情。
相关INFO-NCE的代码可以参考: [腾讯iwiki](https://iwiki.woa.com/pages/viewpage.action?pageId=813543542)

接下来到我们的重头戏: INFO-NCE
首先:INFO-NCE需要面对的问题是，高纬稀疏数据有些数据不能出现在label中的情况，这个时候没有label的那部分数据是浪费掉的。在label较少的时候怎么样把这部分数据利用起来。

这篇[《理解Contrastive Predictive Coding和NCE Loss》](https://zhuanlan.zhihu.com/p/129076690)很不。

在原始的INFO-NCE论文中, 考虑到INFO-NCE主要是用在Auto Encoder的情况。在了是的表征能力更强，我们引入一个predictive任务来预测当前的representation对物品的表示。


对于输入item A我们引入一个任务来学习item A的Embedding接下来序列的预测任务并且计算loss。一个单纯就是用来学习representation的任务和一个可以用来做预测的任务。
在NCE中，从分类转成二分类用来表示representation的结果。这种情况下是不native的。


在NCE中，将负样本的分布服从Zipfian分布。

# 从INFO NCE到Fcoal Loss

在目标检测当中,什么是two-stage detector和one-stage detector.

flocal loss这个函数可以减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。
关注Focal loss对Loss的修改, 我们可以看一下