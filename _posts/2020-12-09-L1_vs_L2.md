---
layout: post
title: L1正则和L2正则的区别
description: "L1正则和L2正则的区别"
modified: 2020-10-01
tags: [Deep Learning, Seq2Seq]
categories: [算法]
image:
  path: /images/algorthim/l1_l2/regular-1.jpeg
  feature: /algorthim/l1_l2/regular-1.jpeg
  credit: x1
  creditlink: https://deeplearn.org/arxiv/69226/a-comparative-study-on-hierarchical-navigable-small-world-graphs
---
目录

* TOC 
{:toc}

在面试中经常会被问到L1正则和L2正则的区别是什么，今天我这边就来做一个简单的梳理。

这篇文章主要解决以下几个问题:

1. 什么是regularization(正则)?
2. 什么是L1正则，什么是L2正则? 怎么样计算这两种正则方式?
3. 这两种正则方式的区别？
4. 什么时候使用L1什么时候使用L2，使用这两种正则方式会导致什么样的模型效果？
 
基于这几个基本问题，我们开始这篇文章的讨论。在这篇文章的第一小节中，
主要会回答问题1以及问题1，第二小节主要回答问题2。在第三小节中对问题3和问题4的内容
做一些深入的探讨。

# 1 正则化

## 1. 什么是正则化

在开始了解什么是正则化之前，我们可以先来看一下为什么需要正则化？

在聊为什么需要正则化之前，那么我们先来聊一聊过拟合。
在机器学习的训练中可能会出现这样的问题:**模型在训练集上的效果非常好，
但是在测试集上的效果很差。** 这种情况，我们称之为过拟合。

如图1所示，我们给出了模型训练中的三种可能状态分别为:

欠拟合: 学习到的模型过于简单，没有办法很好的拟合数据真实分布。

恰好拟合: 学习到的模型能够很好的拟合数据情况。

过拟合: 学习到的模型过于复杂，完全拟合了训练数据，甚至连噪声数据都拟合了，相当于背下了答案。
在训练集上具有较好的效果，在测试集上效果较差。

<div align="center">
<image src="/images/algorthim/l1_l2/regular-2.png"/>
</div>

<div align="center">
图1&nbsp;&nbsp;&nbsp;&nbsp;模型训练中的不同状态
</div>

过拟合换句话说就是随着模型复杂程度的增加，模型的训练误差不断减小，
但是在测试集上的误差并不是不断减小的。随着模型复杂的增加，在测试集
和训练集上的误差变化整体来看如图2。

<div align="center">
<image src="/images/algorthim/l1_l2/regular-3.PNG"/>
</div>

<div align="center">
图3&nbsp;&nbsp;&nbsp;&nbsp;训练误差、测试误差与模型复杂度的关系
</div>

那么也就是说，当我们构建一个复杂模型尤其是构建具有较大参数规模的神经网络的时候，
很容易出现过拟合的现象。

**正则化**:是一种通过对学习算法进行轻微修改加入额外信息，
从而使得模型具有更好的泛化能力，从而减少过拟合的方法。
通过正则化可以提高模型在不可见数据(训练数据外的数据)上的性能。

## 1.2 正则化如何减少过拟合

## 1.3 

# 2 L1正则和L2正则

# 3. 两种正则方式的区别

[[1] An Overview of Regularization Techniques in Deep Learning (with Python code)](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)

[[2] What is the difference between L1 and L2 regularization? How does it solve the problem of overfitting? Which regularizer to use and when?](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)