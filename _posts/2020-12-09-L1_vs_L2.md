---
layout: post
title: L1正则和L2正则的区别
description: "L1正则和L2正则的区别"
modified: 2020-10-01
tags: [Deep Learning, Seq2Seq]
categories: [算法]
image:
  path: /images/algorthim/l1_l2/regular-1.jpeg
  feature: /algorthim/l1_l2/regular-1.jpeg
  credit: x1
  creditlink: https://deeplearn.org/arxiv/69226/a-comparative-study-on-hierarchical-navigable-small-world-graphs
---
目录

* TOC 
{:toc}

在面试中经常会被问到L1正则和L2正则的区别是什么，今天我这边就来做一个简单的梳理。

这篇文章主要解决以下几个问题:

1. 什么是regularization(正则)?
2. 什么是L1正则，什么是L2正则? 怎么样计算这两种正则方式?
3. 这两种正则方式的区别？
4. 什么时候使用L1什么时候使用L2，使用这两种正则方式会导致什么样的模型效果？
 
基于这几个基本问题，我们开始这篇文章的讨论。在这篇文章的第一小节中，
主要会回答问题1以及问题1，第二小节主要回答问题2。在第三小节中对问题3和问题4的内容
做一些深入的探讨。

# 1 正则化

## 1. 什么是正则化

在开始了解什么是正则化之前，我们可以先来看一下为什么需要正则化？

在聊为什么需要正则化之前，那么我们先来聊一聊过拟合。
在机器学习的训练中可能会出现这样的问题:**模型在训练集上的效果非常好，
但是在测试集上的效果很差。** 这种情况，我们称之为过拟合。

如图1所示，我们给出了模型训练中的三种可能状态分别为:

欠拟合: 学习到的模型过于简单，没有办法很好的拟合数据真实分布。

恰好拟合: 学习到的模型能够很好的拟合数据情况。

过拟合: 学习到的模型过于复杂，完全拟合了训练数据，甚至连噪声数据都拟合了，相当于背下了答案。
在训练集上具有较好的效果，在测试集上效果较差。

<div align="center">
<image src="/images/algorthim/l1_l2/regular-2.png"/>
</div>

<div align="center">
图1&nbsp;&nbsp;&nbsp;&nbsp;模型训练中的不同状态
</div>

过拟合换句话说就是随着模型复杂程度的增加，模型的训练误差不断减小，
但是在测试集上的误差并不是不断减小的。随着模型复杂的增加，在测试集
和训练集上的误差变化整体来看如图2。

<div align="center">
<image src="/images/algorthim/l1_l2/regular-3.PNG"/>
</div>

<div align="center">
图3&nbsp;&nbsp;&nbsp;&nbsp;训练误差、测试误差与模型复杂度的关系
</div>

那么也就是说，当我们构建一个复杂模型尤其是构建具有较大参数规模的神经网络的时候，
很容易出现过拟合的现象。

**正则化**:是一种通过对学习算法进行轻微修改加入额外信息，
从而使得模型具有更好的泛化能力，从而减少过拟合的方法。
通过正则化可以提高模型在不可见数据(训练数据外的数据)上的性能。

## 1.2 正则化如何减少过拟合

我们已经介绍了模型训练能够学习到的三种可能状态:欠拟合(high bias)，
拟合，过拟合(high variance)。

可是，过拟合是怎么产生的为什么能够通过正则化来解决过拟合。

过拟合发生的本质原因是由于监督学习问题的不确定性，这种不确定性来源于：从n个(线性无关)
方程中可以解n个变量，解n+1个变量会解不出。
那么在监督学习中数据(对应了方程)远远少于模型空间(对应了变量)。

在这种情况下可以把过拟合问题做如下拆分:

1. 有限的数据集不能完全反应一个模型的好坏，然而我们不得不在有限的数据上挑选模型。
因此，我们完全有可能挑选到在模型训练上表现好但是在测试数据上表现很差的模型。

2. 如果模型空间太大，会有很多的模型可供挑选，那么挑选到对的模型的机会会变小。

3. 与此同时，如果我们要在训练上数据表现足够好。最直接的方法，就是选用足够大的模型空间，
从足够大的模型空间中挑选模型。否则如果模型空间太小，就不存在能够拟合数据很好的模型。

因此，如果我们要更好的拟合训练数据，就要挑选足够大的模型空间。
可是挑选足够大的模型空间之后，挑选到的模型又不一定适用于测试数据。
因为模型空间太大学出来的模型可能千奇百怪。

那么在这种情况下，能够用来缓解过拟合的方法有这么几种:

1. 增大数据量

2. 控制模型空间的复杂度

3. 使用多个模型融合的结果

使用正则化相当于给方程的求解施加了约束条件，减小了模型空间的复杂度。


# 2 L1正则和L2正则

## 2.1 L1正则化

在讨论L1正则的时候，我们需要考虑这么几个问题:

* 1.什么是L1正则?
* 2.L1正则有什么特点?
* 3.什么情况下适合使用L1正则?

**L1正则是指权值向量的绝对值之和，通常表示为** ${\|\|w\|\|}_1$
在添加正则项的时候一般会在正则项前面添加一个系数$\alpha$

在使用L1做正则化的时候，更容易产生稀疏的权值矩阵，
也就是产生一个稀疏的模型，可以用来做特征选择。

在使用L1做正则化的时候，目标函数是非光滑的。对于非光滑的优化问题，
它的最优解要么是在导数为0的地方，要么在不可导的地方，也就是在各个角上。
对于对于L1正则而言，各个“角”对应的位置很多特征系数为0，所以L1会给出一个稀疏解。

根据L1正则化的特点，L0可以用来得到稀疏解，主要用来使得参数稀疏化做特征选择。

>除了L1之外，我们在这里再聊一下L0，表示参数中非0值的个数，也能够使得参数
更为稀疏，但是在实际研究中由于L0很难求解，所以一般使用L1代替。


## 2.2 L2正则化

**L2正则是指权值向量中各元素的平方和然后再求平方根,
L2正则项通常表示为** $\|\|w\|\|_2$，L2正则通常用来防止过拟合。

**那么为什么L2正则可以用来解决过拟合问题?**

在拟合过程中，通常都倾向于让权值尽可能的小，
最后构造一个所有参数都比较小的模型。
一般认为参数值较小的模型比较简单，能适应不同的数据集，
也在一定程度上避免了过拟合现象。

例如，对于一个线性回归方程，若某个参数比较大，
只要稍微有一点点偏离就会对结果造成很大的影响，
但是如果参数足够小，数据的扰动带来的影响会小模型的健壮性会更强一下。

**为什么L2可以获得很小的参数？**

<div align="center">
<image src="/images/algorthim/l1_l2/regular-4.PNG"/>
</div>

<div align="center">
图4&nbsp;&nbsp;&nbsp;&nbsp;L2正则为什么能使得参数变小
</div>

## 2.3 L1和L2正则的区别

* 计算方式的不同

* 求得参数矩阵权重的不同

* 使用范围的不同

[[1] An Overview of Regularization Techniques in Deep Learning (with Python code)](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)

[[2] What is the difference between L1 and L2 regularization? How does it solve the problem of overfitting? Which regularizer to use and when?](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)

[[3] 为什么L1和L2正则化可防止过拟合](https://zhuanlan.zhihu.com/p/85630046)

[[4] 深入理解L1、L2正则化](https://www.cnblogs.com/zingp/p/10375691.html)

[[5] ]()