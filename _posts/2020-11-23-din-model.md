---
layout: post
title: Alibaba DIN模型介绍
description: "简述Alibaba DIN模型原理"
modified: 2020-10-01
tags: [Deep Learning, Seq2Seq]
categories: [算法]
image:
  path: /images/algorthim/din/din-1.jfif
  feature: /algorthim/din/din-1.jfif
  credit: x1
  creditlink: https://deeplearn.org/arxiv/69226/a-comparative-study-on-hierarchical-navigable-small-world-graphs
---
简述Alibaba DIN模型

在这篇文章中主要介绍了一下Alibaba的论文《Deep Interest Network for Click-Through Rate Prediction》

文章中心思想的部分内容主要包括三个部分:

* Attention在CTR预估任务中的使用

* 自适应的正则化方法

* Dice激活函数

在

# 1. DIN的基本思想

# 2. 深度模型的训练技巧

# 3. DIN的实验效果

# 4. DIN的代码实现

1. mini-batch aware regularization

2. data adaptive activation function

用来帮助训练具有上亿参数的工业深度模型。

我们在两个公开的数据集和一个拥有20多亿样本的阿里巴巴真实生产数据集上的实验证明了所提方法的有效性，这个
方法与最先进的方法相比取得了优异的性能。

# 1. INTRODUCTION

在cost-per-click(CPC)广告系统中，广告通过eCPM进行排名。eCPM是点击率和广告价格的乘积，其中点击率由系统预测。
因此，点击率预估模型的效果在广告系统中直接影响了系统的收入，在广告系统中起着关键的作用。CTR建模并进行CTR预估
在工业界和学术界都受到了广泛的关注。

最近收到计算机视觉和自然语言处理中深度学习成功应用的启发，基于深度学习的CTR预估方法在点击率预估任务中被提出。
这些方法遵循Embedding&MLP的范式:将大规模的稀疏特征首先映射到一个低维的向量，然后对向量按照分组转化为固定的
维度，最后将不同组的向量拼接Feed到一个全连接的模型(MLP)当中去来学习不同特征之间的非线性关系。与常用的逻辑
回归模型相比，基于Deep Learning的方法可以较少大量的特征工程的工作，大大提高模型的能力。为了简单起见，在本文
中我们将这些方法称之为Embedding&MLP方法。这些方法现在在CTR预估中变得越来越流行。

然而，在Embedding&MLP方法中用户的向量表示为固定的维度，会对用户的兴趣表达造成瓶颈。以电子商务的广告展示为例。
当用户访问电子商务网站的时候可能同时对多个种类的物品感兴趣。那也就是说用户的兴趣是多样的。在CTR预估任务中，
用户的兴趣通常是通过用户的行为数据来捕获。在Embedding&MLP方法中，对于每一个用户通过把用户的行为转换成欧几里得
空间中一个固定维度的向量来学习用户兴趣的表示。也就是不同用户的多样的兴趣通过一个固定维度的向量来表示。这种方式
限制了向量的Embedding&MLP方法的表征能力。为了使得这个向量能够捕获到更多的用户兴趣表示，这个固定维度的向量
需要做更大的扩展。不幸地是这种情况下将极大地扩大了学习参数的大小增加了过拟合的风险。此外，扩增向量的维度还增加了
计算和存储的负担，这在工业界在线系统上是没有办法被容忍的。

另外一方面，在预测候选广告的时候，没有必要将某个用户的所有不同兴趣压缩到同一个向量中。因为只有部分的用户兴趣
会影响到用户的行为(点击或者不点击)。例如，一个女性游泳爱好者会点击推荐的泳镜，主要是因为购买了的泳衣而不是因为
她上周购买的鞋子。以这个为原始动机我们提出了一个新模型:Deep Interest Network(DIN)。对于给出的广告，DIN通过计算
这个广告和历史行为的相关性来计算用户的向量表示。在DIN中，通过引入一个局部的激活单元通过软搜索来关注跟候选视频相关
的用户行为并加权求平均计算计算得到用户的兴趣表示。与候选广告相关性较高的行为能够获得较高的激活权重，并且主导用户的
兴趣表示。我们在实验部分对这种现象进行了可视化。在这种情况下，用户的向量对于不同的候选广告而言，用户的向量是不同的。
这种方法在有限的维度下提高了模型的表征能力，使得DIN能够更好的捕捉到用户兴趣。

训练工业界中具有大规模稀疏特征的模型是一个较大的挑战。例如，基于SGD的优化方法只更新在每一个mini-batch中出现过的稀疏特征
的参数，如果需要加上传统的l2正则会使得计算没有办法被接受。因为这需要为每一个mini-batch计算整个参数的l2范数。在这篇文章
中，我们提出了一种新的mini-batch感知正则化方法。其中只有出现每个小批量中的非零特征参数参与L2范数的计算，这使得计算可以被接受。

此外，我们设计了一个数据自适应的激活函数。这个激活函数通过自适应调整输入校正点w.r.t的分布来使得PReLU可以通用。
这个方法被证明有助于训练具有稀疏特征的工业网络。

这篇文章的主要贡献我们总结为如下:

* 我们指出了使用固定长度的向量表示用户多样兴趣的缺陷并且提出了一个新的Deep Interest Network。
在DIN中，对于给定的广告，该网络使用一个局部的激活函数从用户的历史行为中自适应地学习用户的兴趣表示。DIN可以
提高模型的表征能力更好的捕获用户的兴趣的多样性。

* 我们提出了两个技术新的技术来帮助训练工业界的深度模型:

1) mini-batch aware regularizer。该方法通过避免了在神经网络训练正则化中大规模的参数计算，并且有效地避免了过拟合。

2) 一个自适应的激活函数，通过考虑输入的分布来广泛地使用PReLU，并且取得了较好的效果。

* 我们在公开数据集和阿里巴巴数据集上进行了广泛的实验，结果验证了我们提出的DIN方法和训练技术是有效的。我们公开了
我们的代码。DIN方法部署在了全球最大的商业展示广告系统中，为业务的发展做出了巨大的贡献。

在这篇文章中，我们主要集中在电商行业广告展示的CTR预估建模。该方法同样的可以应用于具有丰富的用户行为的其他类似场景，
例如基于个性化推荐的电商网站，社交网络中的订阅排名等等。

本文剩下的内容组织如下。在第二个章节中，我们讨论了相关工作并且介绍了。在第三章节中，我们介绍了电子商务网站
展示广告系统用户行为数据的特征和背景。在第四个章节中我们描述了DIN模型的设计的具体细节，以及我们提出的两个训练方法。
在第六节中，我们展示了实验结果，同时在第七节中给出了具体结论。

# 2. RELATED WORK

随着在CTR预估当中，模型的结构由浅入深，CTR预估模型当中使用的样本数量和特征维度也变得越来越大。为了更好地提取特征之间地
关系来提高模型的性能，在模型设计上面主要做了以下几个工作:

作为里程碑式的工作，NNLM学习了每个词汇的向量表示，避免了在自然语言处理中的维度爆炸。我们通常将这种方法称之为嵌入。
受到自然语言处理上大量模型的启发，CTR预估同样需要处理大规模的稀疏特征。

LS-PLM和FM可以视为看成是只有一个隐藏层的模型，这类模型首先在稀疏特征上使用了embedding，然后对需要拟合的目标添加了一些特定的
转换来捕获特征之间的组合关系。

Deep Cross和Wide&Deep以及YouTube DNN推荐的CTR模型扩展了LS-PLM并且将转换函数换成了了MLP网络，这个方法大大地提高了
模型对数据的学习能力。PNN通过在嵌入层之后加上一个product层来捕获高维特征之间的交互。DeepFM将因子分解机作为Wide&Deep
中的Wide模块来避免了特征工程的工作。总之，所有的这些方法都具有相同的结构:embedding的层的拼接(来学习稀疏特征的表示)
和MLP(用来学习特征之间的交叉)。这些模型能够大大地减少人工的特征工程工作。我们的基础模型也是采用这种结构。然后在具有
大规模用户特征的行为当中，特征通常是一系列变长的id列表。例如:在YouTube推荐系统中的搜索历史或者观影历史。这种方法通常
通过累加或者求平均将变长的观影历史转化成固定长度的embedding向量，这种方式会导致信息的损失。在DIN中通过自适应的向量表示
可以学习到用户的特征表示，从而提高模型额表征能力。

注意力机制源于神经机器翻译(NMT)领域。NMT通过对annotation进行加权从而得到可能的下一个annotation，并且只关注跟接下来
的目标词汇有关的信息。

我们公开了我们的代码，此外我们展示了如何使用新提出的可以训练上亿参数的深度网络训练技术如何在全世界最大的广告系统中
部署DIN网络。

# 3 BACKGROUND

在电商网站中，例如Alibaba，广告自然就是对应的商品。在这篇文章中，如果没有特别的声明的话，我们把广告视为商品。在图1中
给出了Alibaba广告展示系统的运行流程。在整个运行流程中主要包括两个部分:

i) matching 阶段使用类似于协同过滤的方法给相关的访问用户生成可能感兴趣的广告。

ii) ranking 阶段对每一个给出的广告预测CTR得分，然后挑选Top的广告展示给用户。


每天会有成百上千的用户访问这个电商网站并且生成了大量的用户行为，这对广告系统构建matching和ranking是至关重要的。
值得一提的是用户的行为包含了多样的兴趣。例如，一个年轻的妈妈最近买了羊毛大衣、T恤、耳环、手提包、皮革手提包
以及儿童外套。这些行为数据给了我们有关于他们购买兴趣的暗示，当他访问电商网站的时候，我们给出这个用户可能感兴趣的
广告给她，例如一个新的手提包。很明显我们展示的兴趣只能契合这个年轻妈妈一部分的兴趣。总之，用户行为中的兴趣是多样的
并且在展示正确的广告的时候可能只激活少部分兴趣，在后文中，我们展示了这些特征在建立CTR预估模型当中的重要作用。

# 4 DEEP INTEREST NETWORK

与意图明确的搜索不同，用户在进入广告系统的时候是没有明确的意图的。所以在构建点击率预估模型的时候
我们需要有效的方法从丰富的观影历史中抽取用户兴趣。在推荐系统的CTR预估中用来描述用户和广告的特征是CTR模型
当中的基本元素，因此使得这些特征可解释并且从这些特征中分析出信息是至关重要的事情。

## 4.1 特征表示

在CTR预估当中使用到的数据大多是多组分类的数据，例如[weekday=Firday, gender=Female, 
visited_cate_ids={Bag,Book}, ad_cate_id=Book]。这些特征通常会基于encoding转化为高维度的稀疏二进制特征。
在对多分类数据进行编码采用one-hot或者multi-hot编码。那么刚才的这个例子最终会编码为如下的格式，
分成以下的四个分组:

<div align="center">
<image src="/images/algorthim/din/din-2.JPG"/>
</div>

在Table 1中给出了需要使用的所有特征。这些所有的数据由四个分类组成，其中用户的行为特征向量是典型的multi-hot编码向量并且包含了丰富的用户行为。值得注意的是
在我们的集合中没有拼接特征。我们通过深度神经网络来捕获特征之间的交叉。

<div align="center">
<image src="/images/algorthim/din/din-3.JPG"/>
</div>

## 4.2 基本模型(Embedding & MLP)

大部分流行的模型采用的模型结构通常都是相同的Embedding&MLP的范式。如图2所示，我们以这样的模型结构为基础，这个模型结构包含以下几个部分:

* **Embedding Layer(Embedding 层)**: 对于高维的输入向量，embedding层将这些向量转化为低维的稠密向量表示。

* **Pooling层和concat层**: 值得注意的是不同的用户具有不同数据的行为。因此，在mutli-hot向量中不同用户的向量维度为1的值不同，那么对应得到的embedding list
是长度不同，是一个变长的量，但是全连接网络只能处理固定长度的输入，这个时候通常的做饭是使用池化层来得到一个固定长度的向量。在池化操作中最常用的方法是
sum pooling和average pooling。sum pooling通常是对所有向量列表进行求和操作，average pooling则是对向量列表进行累加操作。

pooling层和embedding层都是以分组的方式完成特征操作，然后将稀疏特征映射到一个固定维度的向量表示中。当分组生成特征完成后将用户所有的特征向量进行拼接得到
一个总体的特征向量。

* **MLP层**: 对于Pooling层和concat层给出的全局向量，使用全连接层学习特征之间的交叉。在最近提出的方法中，给出了如何使用全连接层更好地捕获特征之间的交叉。

* **LOSS**： 在基本模型中我们采用负对数似然函数作为目标函数，函数形式的具体定义如下:

$$L = - \frac{1}{N} \sum_{(x,y) \in S} (y\;log\;p(x) + (1-y)\;log\;(1-p(x)))$$

其中 $S$ 表示训练数据集 $N$ 表示数据集的大小, $x$ 表示神经网络的输入，$y \in {0, 1}$ 作为模型训练的标签，$p(x)$ 是模型softmax层之后的输出，表示样本 $x$
被点击的概率。

## 4.3 深度兴趣网络的结构

在上面的所有特征中，用户的行为特征是最重要的特征并且在电商应用场景用户的兴趣建模当中起着十分重要的作用。

在基本的模型当中，通过对用户行为特征分组中的所有用户行为做pooling操作来得到一个固定长度的向量表示。这个向量表示对于给定的用于来说是相同的并没有考虑到
给出的广告候选集。在这种情况下，通过一个低维的向量对于用户兴趣多样性的表达存在瓶颈。为了使得用户能够更好地捕获兴趣，最简单地方式就是扩增向量的维度，
然而这种方式会大大提高参数学习的数量。同样的这种情况可能会导致在训练数据集上的过拟合，并且增加计算和存储的负担，这对于线上的系统来基本是不可能忍受的。

有没有什么更优雅的办法在有限的维度下通过一个向量表示用户的多样的兴趣。我们在局部激活特性中找到了灵感并且设计出了一个新的Deep Interest network(DIN)，
想象一下当在第三节中的年轻母亲访问我们的电商系统的时候。她发现展示的新的手提包很可爱然后点击了一下。那么，让我们来解释一下这个点击的理由。广告展示系统
通过对这位母亲的观影历史进行软搜索，发现她最近浏览了手提袋和皮革手袋的类似商品，从而触及了她相关的兴趣。也就是说与展示广告相关的行为对点击动作起了比较
大的贡献。对给出的广告DIN通过局部兴趣表示来模拟这个过程。在DIN中并且通过同一个向量来捕获用户的多样性，DIN通过自适应的关注候选广告相关的观影历史来计算用户的
用户的兴趣向量表示。对于不同的展示广告具有不同的向量表示。

在图2中给出了DIN的结构的插图和基本的模型相比。DIN提出了一个新设计的局部激活单元并且包含了其他相同的结构。激活单元如一个sum pooling使用来自适应地计算
给定候选广告的情况下$A$的用户行为特征向量表示。

$$V_U(A) = f(V_A, e_1, e_2,...,e_H) = \sum_{j=1}^H a(e_j, V_A)e_j = \sum_{j=1}^H w_j e_j$$

其中 ${e_1, e_2,...,e_H}$ 是用户 $U$ 的行为embedding向量列表，大小为 $H$ $V_A$ 是候选广告A的向量，$V_U(A)$ 是随着不同的广告变化的。如图2所示，$a(\cdot)$ 是一个前馈神经
网络，我们把这个神经网络的输出作为用户行为向量激活的权重。在 $a(\cdot)$ 除了将两个向量作为输入之外，模型还将这两个向量的内积输入到后续的相关性建模当中。

在上述方程中给出的本地激活单元和NMT任务中的attention方法使用了相同思想。然而，与传统的attention方法不同，在上述方程中放宽了$\sum_{i}w_i = 1$的约束，
来保留用户兴趣的强度。也就是说对 $a(\cdot)$ 的输出放弃了使用softmax做normalization。此外，$\sum_{i}w_i$在某种程度上被视为用户兴趣激活程度的大体值。例如一个
用户的行为中90%的兴趣是衣服，10%的兴趣爱好是电子产品。如果给出两个候选的广告分别是T-shirt和手机。T-shirt激活了大部分和衣服有关的兴趣，会得到一个比手机更高的
兴趣权重。传统的attention方法对 $a(\cdot)$的输出进行归一化后在数字规模上丢失了分辨能力。

在对用户行为序列做处理的时候，我们也尝试了LSTM来对用户行为做建模，但是结果并没有明显的提升。与自然语言处理任务中受到语法约束的文本不同，用户的行为序列可能包含
多个并发的兴趣。这些用户兴趣的快速跳转和突然结束可能导致这些兴趣看起来更为嘈杂。一个可能的方向就是采用特殊的结构来对行为序列关系进行建模。在这里我们把用户行为序列
的建模留给未来的研究。

# 5 训练技巧
在阿里的广告推荐系统中具有上亿规模的物品和用户。在实际情况中，工业深度模型的训练在处理大规模特征上面临这巨大的挑战。
在这个章节中，我们介绍了两种重要的技术，并且这两种重要的技术并且在实践中证明这两种技术在实践中是效果是有提升的。

## 5.1 小批量感知正则化

过拟合是训练工业界网络中需要面临的重大挑战，例如我们在模型中添加了600万维度的物品id
(包括用户已经观看的物品和表1中已经包含的物品)这样细粒度的特征。
根据Figure 4中深绿色线条显示，如果模型没有正则化的情况下，
迭代完一轮之后模型的性能有明显的下降。在具有稀疏特征并且具有成百上千参数的深度模型中，
又不能直接使用传统模型的正则化方式，例如L1正则和L2正则。以L2正则为例，在采用SGD优化的情况下，
只有在mini-batch中非0的稀疏特征才需要更新参数。然而，当我们添加L2正则的时候，对于每个mini-batch
需要在全局参数上计算L2的模。在这成百上千规模的参数上无疑会导致无法接受的计算量级。

在这个章节中，我们介绍了一种高效的mini-batch感知正则化，这种方法只需要计算每个mini-batch中出现的稀疏
特征的L2模，从而使得计算可以被接受。事实上字典嵌入中的参数占据了CTR模型中的大部分参数。
接下来我们使用$W \in \mathbb{R}^{D \times K}$表示全体的embedding映射参数，其中 $D$ 表示向量的维度，$K$
表示特征空间的维度，我们如果扩展L2正则在采样到的样本上，那么对应的参数计算方式如下:

$$L_2(W) = ||W||^2_2 = \sum_{j=1}^K ||w_j||^2_2 = \sum_{(x,y)\in S} \sum_{j=1}^K \frac{I(x_j\ne0)}{n_j}||w_j||^2_2$$

其中 $w_j\in \mathbb{R}^D$ 是其中的第j条向量，$I(x_j\ne0)$表示如果实例$x$是否有特征id $j$，其中$n_j$表示
特征id $j$ 在所有的样本中出现的次数。上面的这个方程可以转化为下面的这种形式:

$$L_2(W) = \sum_{K}^{j=1}\sum_{B}^{m=1} \sum_{(x,y)\in \mathcal{B_{\mathbb{m}}}} \frac{I(x_j \ne 0)}{n_j} ||w_j||^2_2$$

其中$B$ 表示mini-batch的大小，$\mathcal{B_{\mathrm{m}}}$ 表示第 $m$ 个mini-batch。在上面的公式中，我们让
 $\alpha_{mj} = max_{(x,y)\in \mathcal{B_{\mathbb{m}}}}I(x_j\ne0)$ 表示在mini-batch $\mathcal{B_{\mathrm{m}}}$中
 至少存在一个特征id $j$。那么上面的方程可以大致简化为如下所示:
 
 $$L_2(W) \approx \sum_{j=1}^K \sum_{m=1}^B \frac{\alpha_{mj}}{n_j}||w_j||^2_2$$
 
在这种情况下，我们推断出一个感知mini-batch的L2正则。对于第$m$个mini-batch，对应的梯度计算如下:

$$w_j \leftarrow w_j - \eta[\frac{1}{|\mathcal{B_\mathbb{m}}|} 
\sum_{(x,y)\in \mathcal{B_{\mathbb{m}}}}{\frac{\partial L(p(x),y)}{\partial w_j} + \lambda \frac{\alpha_{mj}}{n_j}w_j}]$$

在上述公式中，只需要对在第$m$个mini-batch中参与计算的参数进行正则化。

## 5.2 数据自适应激活函数

PReLU是一个广泛使用的激活函数，这个激活函数的形式如下:

$$\begin{equation}
f(x)=\left\{
\begin{aligned}
s & \,\,\,\,\,\,\,\,\mathbb{if} \,\,\,\,s > 0 \\
\alpha s & \,\,\,\,\,\,\,\,\mathbb{if} \,\,\,\, s\le 0.
\end{aligned}
= p(s) \cdot s + (1-p(s)) \cdot \alpha s
\right.
\end{equation}$$

其中$s$是激活函数$f(\cdot)$的一个一维输入，$p(s)=I(s>0)$是一个指示函数，这个指示函数控制了函数$f(\cdot)$属于
分段$f(s) = s$ 还是分段 $f(s)=\alpha s$ 分段函数的第二段中$\alpha$是一个需要学习的参数。这里我们称$p(s)$是控制函数。

在Fig 3中左边的图中，我们绘制了激活函数$PReLU$的控制函数。$PReLU$函数在取值为0的时候存在一个硬转折，这个激活函数
没有办法适用于每一层都遵循不同分布的情况。考虑到这种情况，我们设计了一个新的基于数据自适应的激活函数**Dice**，

$$f(s)=p(s)\cdot s + (1-p(s))\cdot \alpha s, \,\,\, p(s)=\frac{1}{1+e^{\frac{s-E[s]}{\sqrt{Var[s] + \epsilon}}}}$$

这个激活函数绘制出的图像如Fig 3中右边的图所示。
其中，在训练阶段$E[s]$和$Var[s]$分别是每个输入的mini-batch的均值和方差。在测试阶段，
$E[s]$和$Var[s]$通过计算整个数据集上的平均值得到。其中$\epsilon$是一个非常小的常量，
在我们训练的过程中被设置为$10^{-8}$

<div align="center">
<image src="/images/algorthim/din/din-5.JPG"/>
</div>

Dice可以看成是PReLU激活函数的一般形式。在Dice中的关键点就是根据输入数据的分布自适应地调整校正点，然后
将这个校正点的值设置为输入数据的平均值。因此Dice平滑地控制了PReLU在两个分段函数之间的切换，当$E(s)=0$
并且$Var[s]=0$的时候。Dice退化成PReLU激活函数。

# 6 EXPERIMENTS

在这个章节中，我们展示了我们的实验细节，包括数据集、评估矩阵、实验设置模型的比较以及对应的分析。我们在两个
空开数据集以及一个在阿里巴巴广告展示系统收集的数据集上进行了实验，实验结果显示我们提出的方法具有很好的效果。
我们公开了我们的实验数据集和代码。

## 6.1 数据集和实验设置

**亚马逊数据集:** 亚马逊数据集包括了亚马逊产品的评论以及产品的元数据作为基准数据集。我们在一个名为Electronics的
子数据集上进行实验，这个数据集包括了19万的用户和6万多物品，以及801个类别168万个样本。在这个数据集中
有丰富的用户行为，每个用户和物品至少有5条评论数据。在这个数据集中数据特征包括了:物品id，类别id，用户评论
的物品id列表，类别id列表。让所有用户的行为表示为$(b_1, b_2, b_3,...,b_k,...,b_n)$，那么我们的任务就是
通过前面$k$个评论的物品预测第$k+1$个物品。训练数据的生成过程中对于每个用户采用$k=1,2,...,n-2$作为评论的物品，
在测试集合中，对于我们给出的$n-1$个物品我们需要预测最后一个物品。在实验阶段，对于所有的模型我们都采用指数衰减
的SGD作为优化器。优化器初始的学习速率是1.0，优化器衰减的速率为0.1。在训练过程中mini-batch的大小是32。

**MovieLens数据集:** MoveLens数据集包括了13万的用户，2.7万部电影，其中包括21个类别和2000万条样本数据。为了使得这个
数据集适用于CTR任务，我们将这个数据集改成二分类数据。原始用户的得分在0到5之间，我们把4分和5分的数据标注为正样本
其他的数据标注为负样本。我们基于userID将数据切分为正样本好负样本。在所有的13万用户上，我们随机挑选
10万用户(大概1447万样本数据)加入到训练数据集中剩下的那3万多的用户(大概包含5530000)放入到测试数据集中。那么我们的
预测任务是基于用户历史的行为是否会对给定的电影的预测得分超过3分(postive label)。在所有的特征中包括电影id、
电影类别id和用户相关的电影列表id，电影类别列表id。我们使用了和亚马逊数据集中相同的优化器和实验设置。

**Alibaba DataSet:** 我们收集了阿里巴巴广告展示系统日志中两周的数据作训练样本和测试数据。训练数据和测试数据
的大小分别为2亿和1千4百万条。对于所有的深度模型，16组特征向量的维度设置为12维。
MLP网络采用$192\cdot 200 \cdot 80 \cdot 2$的网络结构。由于数据量庞大我们将batch size设置为5000并且使用Adam
作为优化器并且使用衰减的学习率，起始的学习率为0.001并且衰减速率为0.9。

所有上面这三个数据集的统计如Table 2中所示:

<div align="center">
<image src="/images/algorthim/din/din-6.JPG"/>
</div>

## 6.2 模型效果比较

* **LR**: 逻辑回归是在深度网络在CTR任务里面广泛使用的浅层网络。我们实现了一个LR作为一个较弱的基线模型。

* **BaseModel**: 这个模型在章节4.2中介绍了，模型采用Embedding&MLP的通用结构是大部分使用深度网络的CTR模型中
的通用结构。

* **Wide&Deep**: 在真实的工业界应用中，Wide&Deep模型被广泛地采用。这个模型包含两个部分:
1) wide模型: 用于完成特征交叉
2) Deep模型: 自动提取特征之间的非线性关系，类似于BaseModel。
在Wide&Deep的Wide模块需要通过专业的特征工程来处理特征。我们遵循论文[10]中的做法，将用户行为和候选集交叉内积
作为wide部分的输入。例如在movieLens数据集中，我们对将用户对电影的评分和候选集的电影做交叉。



* **PNN** PNN可以看成是在embedding层引入了一个Product layer来捕获特征交叉的优化版本。

* **Deep FM**: 在DeepFM中采用因子分解机作为Wide&Deep中的Deep模块，省去的特征工程的工作。

## 6.3 评估指标

在CTR预估领域，AUC是被广泛使用的评估方式。在使用AUC进行评估的时候，使用CTR预测的得到排序结果，然后评估这个排序的好坏。这个排序包括用户间的排序和
用户内的排序。在参考文献[7,13]中介绍了一种基于用户加权的变种AUC，这个AUC通过对用户AUC进行平均求平均来衡量用户内部排序的好坏，
并且这个AUC被证明在广告系统中和线上性能是呈线性相关的。在我们的实验中，采用了这个标准。为了表述的简便我们还是称这个方法为AUC。这个AUC的计算方法如下:

$$AUC=\frac{\sum_{i=1}^{n}\#impression_i \times AUC_i}{\sum_{i=1}^{n}\#impression_i}$$

在上面的公式当中，$$\#impression_i$$和$AUC_i$分别表示第i个用户对应的权重和$AUC$

此外，参考文章[25]我们引入了相对度量来策略模型上的相对提升。对于一个随机的猜测者，对应的AUC的值是0.5。因此对应
的相对提升方式计算如下:

$$RelaImpr = (\frac{AUC(measured model)-0.5}{AUC(base model)-0.5}-1) \times 100\%$$


## 6.4 模型在Amazon数据集和MovieLens数据集上的对比结果

表3中展示了在Amazon数据集以及MovieLens数据集上的对比结果。所有的实验结果，我们都重复了五次并且选取了5次结果的平均值。随机初始化对于AUC结果的影响小于
0.0002。根据实验结果来看，很明显的所有的深度模型都明显地打败了LR模型，这证明了深度模型的强大。PNN和DeepFM因其具有特殊的结果设计从而明显胜过了Wide&Deep。
DIN的表现比所有的对比结果都要好。此外，由于在亚马逊数据集上行为数据比较丰富DIN具有更为明显的效果。我们将这个优势归功于DIN里面局部激活单元的设计。DIN通过
软搜索集中考虑和候选集有关的用户行为信息。通过这种机制，DIN获得了用户兴趣的自适应表示，跟其他模型相比大大提高了模型的表征能力。此外，DIN使用Dice激活
函数能够带来其他的额外提升，证明了我们提出的基于数据自适应的激活函数的高效性。

## 6.5 正则化的表现

由于在Amazon数据集和MovieLens数据集中特征的维度不够是很高，所以我们提出的方法没有出现明显的过拟合现象。然而从阿里巴巴线上系统获取的数据集包含
高维的稀疏特征，所以过拟合会成为一个重要的挑战。例如，当我们使用细粒度特征进行深度模型的训练的时候(例如，物品id特征具有6000万维)的时候，
在没有使用正则化的情况下，数据集上迭代完一轮之后就出现了明显的过拟合，这使得模型的性能如Fig 4中亮绿色线条所示的发生了明显下降。在这种情况下，
我们进行了在几种常用的正则化方法上进行了仔细的实现来测试模型的性能:

* **Dropout:** 随机的丢弃每个样本中50%的特征。

* **Filter**: 过滤访问过的物品id，把访问频率最高的物品过滤出来。在我们的数据集中过滤了top 2000万的物品。

* **Regluarization in DiFacto**. 频繁出现的特征相关的参数被较少的调整。

* **MBA**: 我们提出了Mini-Batch Aware正则化方法。在DiFacto方法和MBA方法中正则化的参数$\lambda$都被设置为0.01。

在Fig4和Table 4中给出了对比的结果。根据Fig4中的具体细节，在第一个epoch里面，具有稀疏物品id特征的数据集上训练的模型的AUC比没有物品id特征的模型要好很多。
然而在训练阶段没有正则化的情况下，过拟合会导致模型的性能快速下降(亮绿色线条)。Dropout可以防止过拟合但是导致了模型的收敛速度变慢。按照频率进行过滤
在一定程度上缓解了过拟合。DiFacto正则对高频的物品id进行了更大的惩罚，训练出的模型性能比按照频率过滤要差。我们提出的MBA方法对比其他方法来说效果要好，
能够明显的防止过拟合。

此外，训练好的模型中带有物品id的表现比不带有物品id的性能更好。因为在细粒度特征中包含了更丰富的信息。考虑到这一点，虽然过滤低频特征的方式比Dropout的方式
模型性能更好，但是因为Dropout的操作丢失了低频的id，从而使得模型没有办法完全的使用细粒度特征。

## 6.6 在阿里数据集上的比较

Table 5中展示了在阿里巴巴全部特征数据集上的实验结果。根据表中结果，我们发现和预期相符，LR明显弱于深度模型。在深度模型上的比较我们得出了下面几个结论:

1. 在具有相同的正则化方法和激活函数的情况下，DIN的表现强于其他深度模型，包括:Base Model，Wide&Deep以及PNN和DeepFM。DIN相对于BaseModel有0.0059的绝对提升
和6.08%的相对提升。这再次验证了局部激活单元结构的有效性。
2. 其次，我们证明了我们在DIN上训练方法的高效性。采用mini-batch aware regularizer的方法对比于dropout的方法有了0.0031的绝对提升。此外基于Dice的方法在PReLU
的基础上带来了0.0015的额外提升。

将所有的效果提升放到一起，使用Dice做激活函数并且带有MBA做正则的情况下整个在基础模型上有了11.65%的相对提升和0.0113的绝对提升。即使和表现最好的DeepFM相比，
DIN也获得了0.009的绝对AUC的提升。需要注意的是在具有数亿流量的商业系统中，0.001的绝对AUC提升的增益也是显著的，并且值得部署到线上。DIN在更好地理解和使用
用户行为数据特性上具有更好的效果。此外，我们提出的两个方法大大的提高的模型的性能帮助我们能够训练工业界的大规模网络。

## 6.7 线上AB测试的结果
我们在2017-05到2017-06进行了仔细的线上实现。在为期一个月的线上实验中，通过使用我们提出的正则化方法和激活函数训练出的最后版本的模型相比于BaseModel而言
贡献了10.0%的CTR和3.8%的RPM的提升。这是一个非常重大的改进，并且证明了我们提出的方法的有效性。现在DIN部署在线上的模型当中并且承担了线上模型中的主要啊流量。

值得一提的是在工业界提供在线的服务并不是一件简单的事情，每天有成百上千万的用户访问我们的系统。甚至更极端的情况，我们的系统没秒需要处理100万的用户请求。
这要求我们的CTR服务具有高的吞吐量和低延时。例如，在我们的实时系统中，对于我们的访问用户我们需要在10ms内预测几百个广告。在我们实际部署的情况下，在CPU-GPU
结构中采用了一下的集中部署技巧:
i) 对请求进行批处理合并相邻的请求，从而发挥GPU的计算优势。
ii) GPU内存使用的优化，以减少在部署中对GPU使用的浪费。
iii) 并行核计算允许多个CUDA并行处理请求。

总之，这些技术的优化使得单台机器QPS的容量实际上增加了一倍。线上的DIN服务也因此受益。

## 6.8 DIN可视化

最后，我们对具体的case进行了研究来揭示在alibaba数据集上DIN的内部结构。我们首先测试了局部激活单元的有效性。Fig5证明了用户行为的激活程度跟候选视频相关。
正如我们预期的那样，与候选集高度相关的行为具有较高的权重。

然后，我们对我们学习到的向量进行了可视化。我们挑选出之前所说的年轻妈妈的样例，随机选取了九个类别(包括衣服，球鞋，包包等等)并且在每个类别中挑选100个
候选的广告。在Fig.6中使用t-SNE展示了通过DIN学习到的物品可视化的结果。在可视化的图像中，我们将相同类别的点设置为相同的形状。我们可以看到相同类别
的数据基本上都属于同一个类下。此外我们对不同的value值设置不同的颜色，Fig 6也是一个热力图，表示了这个母亲的兴趣爱好和空间中潜在的候选集密度分布。
最终的结果展示，DIN可以在候选嵌入空间中形成多个模态的兴趣分布，使得用户能够捕获到他不同的兴趣。

# 7. 结论

本文重点研究了在具有丰富用户行为的电商场景中展示广告的CTR建模任务。在传统深度CTR模型当中，固定长度的向量表示没有办法有效捕获用户兴趣的多样性。为了
提高模型的表达能力，我们提出了一个新的名为DIN的方法，用户对于不同的候选广告能够生成自适应的向量表示。此外我们还引入了两种新的技术来帮助训练工业界的
深度神经网络，并且进一步提高了DIN的性能。这种方法能够很容易地推广到其他的深度学习任务。DIN现在已经部署在阿里巴巴线上的广告系统中。

* **启发能够对不同类进行category activate，然后用在召回逻辑上？然后对不同category再进行召回结果的融合**

[[1] Regularization in Deep Learning — L1, L2, and Dropout](https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)



