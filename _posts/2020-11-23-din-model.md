---
layout: post
title: Alibaba DIN模型介绍
description: "简述Alibaba DIN模型原理"
modified: 2020-10-01
tags: [Deep Learning, Seq2Seq]
categories: [算法]
image:
  path: /images/algorthim/din/din-1.jfif
  feature: /algorthim/din/din-1.jfif
  credit: x1
  creditlink: https://deeplearn.org/arxiv/69226/a-comparative-study-on-hierarchical-navigable-small-world-graphs
---
目录
* TOC 
{:toc}

在这篇文章中主要介绍了一下Alibaba的论文《Deep Interest Network for Click-Through Rate Prediction》

文章中心思想的部分内容主要包括三个部分:

1. Attention在CTR预估任务中的使用

2. 自适应的正则化方法

3. Dice激活函数

在这篇文章中，我们主要从以下几个方面综合的概括一下《Deep Interest Network for Click-Through Rate Prediction》
这篇论文的中心思想以及这篇论文的启发。

在论文的第一小节中，我们主要介绍了Deep Interest Network引入Attention的动机和基本思想，
以及引入Attention之后神经网络的基本结构。在第二节中，总结了自适应正则化和Dice激活函数
在神经网络模型训练上的优化。
最后我们分析了一下阿里公开的Deep Interest Network的实现代码。

# 1. DIN的基本思想

## 1.1 模型结构

在了解Deep Interest Network的基本思想之前，我们首先需要明确Deep Interest Network
需要处理的问题是排序问题，以及排序系统在推荐系统中所处的阶段。

<div align="center">
<image src="/images/algorthim/din/din-model-1.JPG"/>
</div>

<div align="center">
图1&nbsp;&nbsp;&nbsp;&nbsp;推荐/搜索系统架构
</div>

在图1中给出了一个搜索系统的架构和搜索类似的，推荐过程和搜索过程的区别
在于推荐系统的召回阶段和排序阶段的输入是用户画像和场景信息，搜索系统输入
的是用户自主输入的Query信息。

在推荐系统中召回和排序阶段分别完成不同的工作:

**Matching/Retrieval:** 召回阶段根据对应的行为输入从大规模(百万甚至前往量级)的候选集中挑选出成百上千个
用户可能感兴趣的物品，然后送入到排序阶段。

**Ranking**: 排序阶段的输入是用户行为信息和召回阶段的结果，然后给召回阶段的结果准确的排序得分。

Deep Interest Network所需要解决的问题在图1中红框标识的Ranking阶段。
**不同阶段的数据情况和目的(所要解决的问题)决定了我们能够使用什么样的方法。**

排序阶段面临的候选集个数比较小，需要对candidate有准确的区分并且尽可能兼顾排序结果的多样性，但是
对于同一个用户而言，能够对于同类目的物品具有相近的排序得分的可能性较大，因为相近的物品具有相同的特征。

<div align="center">
<image src="/images/algorthim/din/din-model-2.JPG"/>
</div>

<div align="center">
图2&nbsp;&nbsp;&nbsp;&nbsp;Base Model模型结构
</div>

以Base Model的Embedding & MLP的网络结构为例，模型输入通过将用户特征和物品特征concat & flatten输入到
MLP的模型结构中。同一用户具有相同的用户向量的输入，那么模型对同类型的物品的得分可能具有相近的得分。

可是在实际情况当中用户可能具备多个兴趣并且需要模型能够捕获用户的多样兴趣，对多种不同品类的物品都能
给出较为合适的排序得分。

在Deep Interest Network中，举了一个年轻妈妈的例子:

例如一个年轻的妈妈最近购买了羊毛大衣、T恤、耳环、手提包、皮革手提包 以及儿童外套。
这些数据隐含了用户的行为兴趣。如果当这个年轻的妈妈访问推荐系统的时候，可能会给她
推荐一个新的手提包。可是这个推荐结果只是契合了这个年轻母亲的一部分兴趣。

为了使得模型能够捕获用户多样的兴趣，在模型结构中引入了Attention机制。

<div align="center">
<image src="/images/algorthim/din/din-model-3.JPG"/>
</div>

<div align="center">
图3&nbsp;&nbsp;&nbsp;&nbsp;DEEP INTEREST NETWORK
</div>

在DIN中为了捕获用户的不同兴趣，对Embedding & MLP的结构进行了一些修改。在embedding层
生成用户向量的时候，针对每一个candidate采用局部激活函数生成用户不同行为的权重，然后采用sum
pooling生成用户的行为向量。

## 1.2 特征输入

在Alibaba的Deep Interest Network中输入的特征如表1所示。在排序模型中主要的输入为分组或者分类
信息包括: 用户画像特征(User Profile Features)，用户行为特征(User Behavior Features)， 广告特征(Ad Features)
以及场景特征(Context Features)。

这些不同特征通过one-hot或者multi-hot的方式的编码方式编码成为二进制的向量，作为模型的输入，如图4所示。

<div align="center">
<image src="/images/algorthim/din/din-model-4.JPG"/>
</div>

<div align="center">
图4&nbsp;&nbsp;&nbsp;&nbsp;用户特征的编码表示
</div>

## 1.3 局部激活单元

Deep Interest Network本质上是在Base Model上的优化。Base Model是一个常见的DNN模型，主要分为4个部分:

**Embedding层**: 对高维的二进制输入向量，embedding层将这些向量转成低维的稠密向量表示。

**Pooling & Concat层**: 因为不同的用户具有不同的行为，所以在multi-hot的向量里面值为1的位置数目是不同的。例如
一个用户有3次购买行为和5次购买行为的multi-hot为1的索引位置分别为3个和5个。这个时候不同的用户输入经过
embedding层得到的特征向量维度是不同的。Pooling & Concat层的目的就是为了对物品和用户得到固定长度的向量表示。

在Pooling操作中，常用的操作是sum pooling(对应维度求和)和average pooling(对应维度求均值)

**MLP层**: MLP层是对Pooling & Concat层给出的向量通过全连接层来捕获特征之间的交叉。

**LOSS**: 在Base Model中采用负对数似然函数作为损失函数，对损失的计算如下:

$$
\begin{equation}
L = - \frac{1}{N} \sum_{(x,y) \in S} (y\;log\;p(x) + (1-y)\;log\;(1-p(x))) \tag{1.1}
\end{equation}
$$

在Deep Interest Network的动机中认为Pooling & Concat通过简单的pooling操作和concat操作生成
固定长度的向量没有办法有效地捕获到多样的用户兴趣。DIN引入Attention机制做局部激活在对行为
特征进行sum pooling的时候不同行为设置了不同的权重。

假设个定一个用户$A$那么这个用户的向量表示如公式(1.2)

$$
\begin{equation}
V_U(A) = f(V_A, e_1, e_2,...,e_H) = \sum_{j=1}^H a(e_j, V_A)e_j = \sum_{j=1}^H w_j e_j \tag{1.2}
\end{equation}
$$

其中:

${e_1, e_2,...,e_H}$ 是用户 $U$ 的行为embedding向量列表，用户行为向量维度的大小为 $H$。

$V_A$ 是候选广告A的向量，$V_U(A)$ 是随着不同的候选广告变化的。

图2中的$a(\cdot)$ 是一个前馈神经网络，DIN将这个神经网络的输出作为用户行为向量激活的权重。
在 $a(\cdot)$ 除了将两个向量作为输入之外，模型还将这两个向量计算的结果输入到后续的相关性建模当中，
来引入隐含的额外信息来进行相关性建模。

# 2. 深度模型的训练技巧

在DIN中介绍了两种模型训练的技巧分别为:**Mini-batch Aware Regularization**和**Data Adaptive Activation Function**。
我这里翻译为小批量感知正则化和局部自适应激活，在2.1小节和2.2小节中分别介绍一下这两种方法。

## 2.1 小批量感知正则化

在DIN中因为类似于物品id这样细粒度特征的引入很容易出现深度模型的过拟合，并且在文中
给出了图像证明，模型在经过一轮迭代训练后，训练集上的loss明显下降，测试集上loss和auc开始明显提升。


在传统的机器学习模型训练当中解决过拟合问题常用的方法是正则化。可是在深度模型中，
具有大规模的权重矩阵如果使用传统的正则化方式例如L1正则和L2正则会使得模型训练阶段的计算量大大增加。

以L2正则为例，在深度模型的训练中如果采用随机梯度下降作为优化器，
参数更新的时候只需要更新每一个mini-batch中稀疏特征非0值所对应的参数。
可是如果我们需要在模型训练过程中添加L2正则，
那么对每一个mini-batch的参数需要在全局参数上计算L2的模。
这样的计算量级在生产环境是没有办法被接受的。

在DIN的模型训练中引入了mini-batch感知正则的概念，只需要计算每个mini-batch稀疏特征L2
的模型，来减小计算量。

在深度网络的参数中实际上Embedding生成的特征向量占据了大部分参数。在一下描述中，我们使用
$W \in \mathbb{R}^{D \times K}$表示特征嵌入矩阵，其中D是嵌入后特征向量的维度，
K表示输入到神经网络中特征的维度。这种情况下如果我们只是在样本上计算L2正则，那么计算公式如公式2.1

$$
\begin{equation}
L_2(W) = ||W||^2_2 = \sum_{j=1}^K ||w_j||^2_2 = \sum_{(x,y)\in S} \sum_{j=1}^K \frac{I(x_j\ne0)}{n_j}||w_j||^2_2 \tag{2.1}
\end{equation}
$$

其中 $w_j\in \mathbb{R}^D$ 是特征嵌入矩阵中的第j条向量，$I(x_j\ne0)$表示如果样本$x$是否有特征$j$，
其中$n_j$表示特征$j$ 在所有的样本中出现的次数。上面的这个方程可以转化为下面的这种形式:

$$
\begin{equation}
L_2(W) = \sum_{K}^{j=1}\sum_{B}^{m=1} \sum_{(x,y)\in \mathcal{B_{\mathbb{m}}}} \frac{I(x_j \ne 0)}{n_j} ||w_j||^2_2 \tag{2.2}
\end{equation}
$$

其中$B$ 表示mini-batch的大小，$\mathcal{B_{\mathrm{m}}}$ 表示第 $m$ 个mini-batch。在上面的公式中，我们让
 $\alpha_{mj} = max_{(x,y)\in \mathcal{B_{\mathbb{m}}}}I(x_j\ne0)$ 表示在mini-batch $\mathcal{B_{\mathrm{m}}}$中
 是否存在特征$j$。那么上面的方程可以大致简化为如下所示:
 
$$
\begin{equation}
L_2(W) \approx \sum_{j=1}^K \sum_{m=1}^B \frac{\alpha_{mj}}{n_j}||w_j||^2_2 \tag{2.3}
\end{equation}
$$
 
在这种情况下，我们推导出一个小批量感知的L2正则。这个时候对于第$m$个mini-batch，对特征$j$的权重更新公式如下。

$$
\begin{equation}
w_j \leftarrow w_j - \eta[\frac{1}{|\mathcal{B_\mathbb{m}}|} 
\sum_{(x,y)\in \mathcal{B_{\mathbb{m}}}}{\frac{\partial L(p(x),y)}{\partial w_j} + \lambda \frac{\alpha_{mj}}{n_j}w_j}] \tag{2.4}
\end{equation}
$$

在计算公式2.4中只要对第$m$个mini-batch中出现的特征参数进行正则化。

## 2.2 局部自适应激活

# 3. DIN的代码实现

