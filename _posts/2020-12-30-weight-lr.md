---
layout: post
title: YouTube中Weight Logistic Regression 的介绍和使用
description: ""
modified: 2020-10-01
tags: [Deep Learning, Seq2Seq]
categories: [算法]
image:
  path: /images/algorthim/weight_lr/1.png
  feature: algorthim/weight_lr/1.png
  credit: x1
  creditlink: https://deeplearn.org/arxiv/69226/a-comparative-study-on-hierarchical-navigable-small-world-graphs
---
目录

* TOC 
{:toc}

# 1. Weight LR的原理理解

在常见的LR模型当中，假设数据服从伯努利分布，当某件事情发生，
认为其概率为p，那么这件事情不发生的概率为1-p，对应的几率比(odd)为：

$$odds = \frac{p}{1-p}$$

> 几率比(odds):是指一个事件发生的概率与不发生概率的比值。

对几率比求对数，并且将几率比作为输入特征的线性表达式，那么可以得到：

$$logit(\frac{p}{1-p}) = w^T x$$

这个时候得到:

$$Odds = e^{w^Tx}$$

这个时候我们根据概率p，再去推导出$logit$函数的反函数，得到的就是sigmoid函数:

$$\phi(x) = \frac{1}{1+e^{-w^T x}}$$

# 2. YouTube DNN中的使用

在短视频的CTR预估当中，点击发生的概率就是发生点击的视频个数/总曝光的视频个数。
假设发生点击的视频个数为M，公共曝光的视频个数为N，则$p$:

$$p=\frac{M}{N}$$

这个时候可以得到：

$$Odds = \frac{\frac{M}{N}}{\frac{N-M}{N}} =
 \frac{\frac{M}{N}}{1-\frac{M}{N}}$$
 
在这种情况下，我们对正样本添加权重$w_i$，那么上述公式的表述如下:

$$Odds(i) = \frac{w_i \times \frac{M}{N}}{1-w_i \times \frac{M}{N}} = \frac{w_i p}{1-w_ip}$$

在YouTube DNN的推荐中，关键的地方在于正样本权重的选择，
在YouTube DNN中使用了播放时长$T_i$作为权重。
由于在视频推荐场景中，用户打开一个视频的概率$p$通常是一个很小的值，
因此，上面的公式可以继续简化：

$$Odds(i) = w_ip = t_ip$$

由于p就是用户打开该视频的概率，$T_i$是观看的时长，
因此$T_i \times p$就是用户观看某视频的期望时长。
这里有一个好处就是，在CTR进行Serving的时候，
我们只关注相对位置，不关注绝对值，这种情况下只需要计算$Odds$就可以啦!
也就是只是需要计算$e^{W^T x}$，
这样就转化成了根据观看某视频的期望时长进行排序。

# 3. Weight LR的应用实现

针对YouTube DNN中的weight LR，可以通过两种方法实现:

1. up-sampling方式

通过up-sampling根据视频的播放时长增加播放时长长的正样本的数量，
提高训练数据中正样本的权重。

2. magnify weight方式

up-sampling通过改变样本数据的分布，
来提高播放时间较长的视频正样本在训练中所占的比重，
从而起到weight LR应该达到的效果。

除了使用up-sampling的方式之外，
还可以在计算损失的时候引入权重，计算交叉熵。

在TensorFlow中的`tf.nn.weighted_cross_entropy_with_logits`API中实现了
加权交叉熵的计算。

TensorFlow提供的接口如下:

```python
tf.nn.weighted_cross_entropy_with_logits(
    labels, logits, pos_weight, name=None
)
```

* labels: 是样本对应的标签
* logits: 是样本预测概率的对数比
* pos_weight: 表示正样本的权重
* name: 操作名称

在常见的二分类问题中，原本交叉熵的计算方式如下:

$$loss = \frac{1}{N} \sum_{i} -[y_i \cdot log(p_i) + (1-y_i) \cdot (1-p_i)]$$

在Weight LR中二分类问题loss的计算如下:

$$loss = \frac{1}{N} \sum_{i} -[y_i \cdot log(p_i) \cdot w_i + (1-y_i) \cdot (1-p_i)]$$

这里值得注意的一点是，在交叉熵损失里面只是对正样本最weight加权，
不对负样本做加权。

[[1] 非均衡数据处理--如何学习？](https://zhuanlan.zhihu.com/p/34782497)

[[2] tf.nn.weighted_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits)

[[3] 使用带权重交叉熵损失函数定向提升模型的召回率](https://zhuanlan.zhihu.com/p/71648578)

[[4] weighted—-LR的理解与推广](https://www.cnblogs.com/hellojamest/p/11871108.html)

[[5] Notes On Youtube Dnn](https://wuciawe.github.io/machine%20learning/math/2019/05/15/notes-on-youtube-dnn.html)

[[6] 分类机器学习中，某一标签占比太大（标签稀疏），如何学习？](https://www.zhihu.com/question/372186043/answer/1089033462)





